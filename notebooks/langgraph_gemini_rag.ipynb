{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e7e1ee",
   "metadata": {},
   "source": [
    "# LangGraph-style RAG Q&A Agent (Gemini-only, clean & clear)\n",
    "\n",
    "This notebook demonstrates a small Retrieval-Augmented Generation (RAG) pipeline with a 4-node workflow: `plan` -> `retrieve` -> `answer` -> `reflect`.\n",
    "\n",
    "It uses: ChromaDB (local), HuggingFace sentence-transformers for embeddings, and Google Gemini (via `google.generativeai`) as the LLM. The notebook includes minimal logging/prints so you can see each step's outputs.\n",
    "\n",
    "Notes: set `GEMINI_API_KEY` environment variable, or set `GOOGLE_APPLICATION_CREDENTIALS` for service account JSON before running the `answer` cell that calls Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170e09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\jella\\swarm\\notebooks\n",
      "DATA_DIR = c:\\Users\\jella\\swarm\\notebooks\\data\n",
      "CHROMA_DIR = c:\\Users\\jella\\swarm\\notebooks\\chroma_db\n",
      "EMBEDDING_MODEL = sentence-transformers/all-MiniLM-L6-v2\n",
      "USE_GEMINI = True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "print('Working directory:', os.getcwd())\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger('gemini_rag_notebook')\n",
    "\n",
    "DATA_DIR = os.environ.get('RAG_DATA_DIR', os.path.join(os.getcwd(), 'data'))\n",
    "CHROMA_DIR = os.environ.get('RAG_CHROMA_DIR', os.path.join(os.getcwd(), 'chroma_db'))\n",
    "EMBEDDING_MODEL = os.environ.get('RAG_EMBEDDING_MODEL', 'sentence-transformers/all-MiniLM-L6-v2')\n",
    "GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY', '')\n",
    "USE_GEMINI = bool(GEMINI_API_KEY) or bool(os.environ.get('GOOGLE_APPLICATION_CREDENTIALS'))\n",
    "\n",
    "print('DATA_DIR =', DATA_DIR)\n",
    "print('CHROMA_DIR =', CHROMA_DIR)\n",
    "print('EMBEDDING_MODEL =', EMBEDDING_MODEL)\n",
    "print('USE_GEMINI =', USE_GEMINI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "706f2350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data files ready:\n",
      "['c:\\\\Users\\\\jella\\\\swarm\\\\notebooks\\\\data\\\\renewable_energy_overview.txt', 'c:\\\\Users\\\\jella\\\\swarm\\\\notebooks\\\\data\\\\solar_wind_benefits.txt']\n"
     ]
    }
   ],
   "source": [
    "# Create sample data files if they don't exist so the demo can run quickly.\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "sample1 = os.path.join(DATA_DIR, 'renewable_energy_overview.txt')\n",
    "sample2 = os.path.join(DATA_DIR, 'solar_wind_benefits.txt')\n",
    "if not os.path.exists(sample1):\n",
    "    with open(sample1, 'w', encoding='utf-8') as f:\n",
    "        f.write('Renewable energy, including solar and wind, provides low-carbon electricity, reduces greenhouse gas emissions, and diversifies energy supply. Renewables can lower energy costs in the long term and create local jobs.')\n",
    "if not os.path.exists(sample2):\n",
    "    with open(sample2, 'w', encoding='utf-8') as f:\n",
    "        f.write('Solar power generates electricity from sunlight using photovoltaic cells or concentrated solar power; wind power harnesses wind with turbines. Both technologies reduce reliance on fossil fuels and improve air quality.')\n",
    "\n",
    "print('Sample data files ready:')\n",
    "print(glob.glob(os.path.join(DATA_DIR, '*.txt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cdca47",
   "metadata": {},
   "source": [
    "## Ingest documents into ChromaDB\n",
    "The next cell reads .txt files under `./data`, splits them into chunks, creates embeddings with HuggingFace `all-MiniLM-L6-v2`, and stores them in a local Chroma database under `./chroma_db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1347819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 16:55:25,595 INFO Ingesting documents from c:\\Users\\jella\\swarm\\notebooks\\data\n",
      "2025-11-05 16:55:25,601 INFO Creating embeddings using sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-11-05 16:55:25,603 INFO Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-11-05 16:55:25,842 INFO Use pytorch device: cpu\n",
      "2025-11-05 16:55:25,852 ERROR Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-11-05 16:55:25,857 ERROR Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-11-05 16:55:25,929 INFO Indexed 2 chunks into Chroma.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion returned: OK\n"
     ]
    }
   ],
   "source": [
    "def ingest_documents(data_dir: str = DATA_DIR, chroma_dir: str = CHROMA_DIR, embedding_model: str = EMBEDDING_MODEL):\n",
    "    from typing import List, Dict\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "    logger.info('Ingesting documents from %s', data_dir)\n",
    "    patterns = [os.path.join(data_dir, '**', '*.txt'), os.path.join(data_dir, '**', '*.md'), os.path.join(data_dir, '**', '*.csv')]\n",
    "    files: List[str] = []\n",
    "    for p in patterns:\n",
    "        files.extend(glob.glob(p, recursive=True))\n",
    "\n",
    "    if not files:\n",
    "        logger.warning('No documents found in %s. Add files or use Kaggle helper.', data_dir)\n",
    "        return None\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "    texts: List[str] = []\n",
    "    metadatas: List[Dict[str, str]] = []\n",
    "\n",
    "    for path in sorted(files):\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                raw = f.read()\n",
    "        except Exception as e:\n",
    "            logger.warning('Failed to read %s: %s', path, e)\n",
    "            continue\n",
    "        chunks = splitter.split_text(raw)\n",
    "        texts.extend(chunks)\n",
    "        metadatas.extend([{'source': os.path.basename(path)} for _ in chunks])\n",
    "\n",
    "    if not texts:\n",
    "        logger.warning('No text after splitting; aborting ingestion.')\n",
    "        return None\n",
    "\n",
    "    logger.info('Creating embeddings using %s', embedding_model)\n",
    "    hf_embed = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "\n",
    "    vectordb = Chroma.from_texts(\n",
    "        texts=texts,\n",
    "        embedding=hf_embed,\n",
    "        metadatas=metadatas,\n",
    "        collection_name='gemini_rag_docs',\n",
    "        persist_directory=chroma_dir,\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    logger.info('Indexed %d chunks into Chroma.', len(texts))\n",
    "    return vectordb\n",
    "\n",
    "vectordb = ingest_documents()\n",
    "print('Ingestion returned:', 'OK' if vectordb else 'No vectordb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f42625bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kaggle dataset helper (optional)\n",
    "from typing import Optional\n",
    "def download_kaggle_dataset(dataset: str, target_dir: Optional[str] = None, unzip: bool = True) -> bool:\n",
    "    try:\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    except Exception:\n",
    "        print(\"Kaggle api not available. Install 'kaggle' and configure credentials if you need this.\")\n",
    "        return False\n",
    "    api = KaggleApi()\n",
    "    try:\n",
    "        api.authenticate()\n",
    "    except Exception as e:\n",
    "        print('Kaggle auth failed:', e)\n",
    "        return False\n",
    "    td = target_dir or DATA_DIR\n",
    "    os.makedirs(td, exist_ok=True)\n",
    "    print(f'Downloading {dataset} to {td} ...')\n",
    "    api.dataset_download_files(dataset, path=td, unzip=unzip, quiet=False)\n",
    "    print('Done.')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b792c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent workflow nodes: plan, retrieve, answer, reflect\n",
    "from typing import List, Dict\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def plan_node(question: str) -> str:\n",
    "    logger.info('[PLAN] Question received: %s', question)\n",
    "    query = question.strip()\n",
    "    logger.info('[PLAN] Retrieval query: %s', query)\n",
    "    print('[PLAN] Query:', query)\n",
    "    return query\n",
    "\n",
    "def retrieve_node(query: str, chroma_dir: str = CHROMA_DIR, embedding_model: str = EMBEDDING_MODEL, k: int = 4) -> List[Dict[str, str]]:\n",
    "    logger.info('[RETRIEVE] Querying Chroma for: %s', query)\n",
    "    hf_embed = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "    vectordb = Chroma(persist_directory=chroma_dir, embedding_function=hf_embed, collection_name='gemini_rag_docs')\n",
    "    docs = vectordb.similarity_search(query, k=k)\n",
    "    results = [{'source': getattr(d, 'metadata', {}).get('source', 'unknown'), 'content': d.page_content} for d in docs]\n",
    "    logger.info('[RETRIEVE] Retrieved %d documents', len(results))\n",
    "    print('[RETRIEVE] Sources:', sorted({r['source'] for r in results}))\n",
    "    return results\n",
    "\n",
    "def answer_node(question: str, retrieved: List[Dict[str, str]]) -> str:\n",
    "    try:\n",
    "        import google.generativeai as genai\n",
    "    except Exception:\n",
    "        return 'google.generativeai not installed.'\n",
    "    if not USE_GEMINI:\n",
    "        return 'Gemini not configured; set GEMINI_API_KEY or GOOGLE_APPLICATION_CREDENTIALS.'\n",
    "    if GEMINI_API_KEY:\n",
    "        genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "    context_blocks = []\n",
    "    for i, doc in enumerate(retrieved, start=1):\n",
    "        src = doc.get('source', 'unknown')\n",
    "        content = doc.get('content', '').strip()\n",
    "        context_blocks.append(f'[Source {i}: {src}]\\n{content}')\n",
    "    context_text = '\\n\\n---\\n\\n'.join(context_blocks) if context_blocks else ''\n",
    "\n",
    "    system_instruction = (\n",
    "        'You are a helpful, factual assistant. Answer the user question using only the provided context. '\n",
    "        'If the answer is not present in the context, say you cannot answer based on the documents.'\n",
    "    )\n",
    "    user_prompt = f'Question: {question}\\n\\nContext:\\n{context_text}\\n\\nProvide a concise answer and cite source numbers (e.g., [Source 1]).'\n",
    "    full_prompt = system_instruction + '\\n\\n' + user_prompt\n",
    "\n",
    "    preferred_order = [\n",
    "        'gemini-1.5-flash-latest',\n",
    "        'gemini-1.5-pro-latest',\n",
    "        'gemini-pro',\n",
    "        'gemini-1.0-pro',\n",
    "    ]\n",
    "    try:\n",
    "        available = [\n",
    "            m.name for m in genai.list_models()\n",
    "            if 'generateContent' in (getattr(m, 'supported_generation_methods', []) or [])\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        available = []\n",
    "    def short(n: str) -> str:\n",
    "        return n.split('/')[-1]\n",
    "    available_sorted = sorted(available, key=lambda n: (preferred_order.index(short(n)) if short(n) in preferred_order else 999, n))\n",
    "\n",
    "    last_error = None\n",
    "    for model_name in available_sorted or ['models/gemini-pro']:\n",
    "        logger.info('[ANSWER] Trying model %s', model_name)\n",
    "        try:\n",
    "            model = genai.GenerativeModel(model_name)\n",
    "            response = model.generate_content(full_prompt)\n",
    "            answer_text = getattr(response, 'text', None) or str(response)\n",
    "            print('[ANSWER]')\n",
    "            print(answer_text)\n",
    "            return answer_text\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            continue\n",
    "    return f\"LLM call failed; tried {available_sorted or ['models/gemini-pro']}: {last_error}\"\n",
    "\n",
    "\n",
    "def reflect_node(question: str, answer: str, retrieved: List[Dict[str, str]]):\n",
    "    question_terms = [w.lower().strip(',.?') for w in question.split() if len(w) > 3][:12]\n",
    "    coverage_in_answer = sum(1 for t in question_terms if t in (answer or '').lower())\n",
    "    coverage_in_retrieved = sum(1 for t in question_terms if any(t in doc['content'].lower() for doc in retrieved))\n",
    "    result = {\n",
    "        'question_terms': question_terms,\n",
    "        'coverage_in_answer': coverage_in_answer,\n",
    "        'coverage_in_retrieved': coverage_in_retrieved,\n",
    "        'pass': (coverage_in_answer > 0 or coverage_in_retrieved > 0),\n",
    "    }\n",
    "    logger.info('[REFLECT] %s', result)\n",
    "    print('[REFLECT]', json.dumps(result, indent=2))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03df1019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 16:55:35,518 INFO [PLAN] Question received: What are the benefits of renewable energy?\n",
      "2025-11-05 16:55:35,519 INFO [PLAN] Retrieval query: What are the benefits of renewable energy?\n",
      "2025-11-05 16:55:35,520 INFO [RETRIEVE] Querying Chroma for: What are the benefits of renewable energy?\n",
      "2025-11-05 16:55:35,522 INFO Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RUNNING AGENT ---\n",
      "[PLAN] Query: What are the benefits of renewable energy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 16:55:35,761 INFO Use pytorch device: cpu\n",
      "2025-11-05 16:55:35,770 ERROR Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-11-05 16:55:35,776 ERROR Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "2025-11-05 16:55:35,816 INFO [RETRIEVE] Retrieved 4 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RETRIEVE] Sources: ['renewable_energy_overview.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 16:55:36,440 INFO [ANSWER] Trying model models/gemini-2.0-flash\n",
      "2025-11-05 16:55:38,063 INFO [REFLECT] {'question_terms': ['what', 'benefits', 'renewable', 'energy'], 'coverage_in_answer': 2, 'coverage_in_retrieved': 2, 'pass': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ANSWER]\n",
      "Renewable energy provides low-carbon electricity, reduces greenhouse gas emissions, diversifies energy supply, can lower energy costs in the long term, and creates local jobs [Source 1, Source 2, Source 3, Source 4].\n",
      "\n",
      "[REFLECT] {\n",
      "  \"question_terms\": [\n",
      "    \"what\",\n",
      "    \"benefits\",\n",
      "    \"renewable\",\n",
      "    \"energy\"\n",
      "  ],\n",
      "  \"coverage_in_answer\": 2,\n",
      "  \"coverage_in_retrieved\": 2,\n",
      "  \"pass\": true\n",
      "}\n",
      "\n",
      "--- SUMMARY ---\n",
      "Question: What are the benefits of renewable energy?\n",
      "\n",
      "Answer:\n",
      " Renewable energy provides low-carbon electricity, reduces greenhouse gas emissions, diversifies energy supply, can lower energy costs in the long term, and creates local jobs [Source 1, Source 2, Source 3, Source 4].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One-shot run through the pipeline\n",
    "question = 'What are the benefits of renewable energy?'\n",
    "print('\\n--- RUNNING AGENT ---')\n",
    "q = question.strip()\n",
    "query = plan_node(q)\n",
    "retrieved = retrieve_node(query)\n",
    "answer = answer_node(q, retrieved)\n",
    "reflect = reflect_node(q, answer, retrieved)\n",
    "\n",
    "print('\\n--- SUMMARY ---')\n",
    "print('Question:', q)\n",
    "print('\\nAnswer:\\n', answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
